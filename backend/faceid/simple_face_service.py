"""
–ü—Ä–æ—Å—Ç–æ–π —Å–µ—Ä–≤–∏—Å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ª–∏—Ü –∏—Å–ø–æ–ª—å–∑—É—è OpenCV
–†–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ InsightFace - —Ç–æ–ª—å–∫–æ —Å Haar Cascades + –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã
"""
import numpy as np
import cv2
from typing import List, Tuple, Optional, Dict, Any
from datetime import datetime
import logging

from .config import (
    THRESHOLD_HIGH_CONFIDENCE, THRESHOLD_MEDIUM_CONFIDENCE,
    MIN_FACE_SIZE, MAX_FACES_PER_IMAGE
)
from .schemas import VerdictEnum, LivenessEnum
from .utils import (
    normalize_embedding, cosine_similarity, assess_lighting_quality,
    assess_motion_blur, embedding_to_binary, binary_to_embedding
)
from .liveness_detection import liveness_detector
from .models import Person, FaceEmbedding

logger = logging.getLogger(__name__)


class SimpleFaceRecognitionService:
    """–ü—Ä–æ—Å—Ç–æ–π —Å–µ—Ä–≤–∏—Å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Å OpenCV"""
    
    def __init__(self):
        self.face_cascade = None
        self.model_loaded = False
        self._initialize_model()
    
    def _initialize_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenCV"""
        try:
            logger.info("üöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º OpenCV Haar Cascade...")
            cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            self.face_cascade = cv2.CascadeClassifier(cascade_path)
            
            if self.face_cascade.empty():
                logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å Haar cascade")
                return
            
            self.model_loaded = True
            logger.info("‚úÖ OpenCV –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            self.model_loaded = False
    
    def detect_faces(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """–î–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü"""
        if not self.model_loaded:
            raise RuntimeError("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        faces = self.face_cascade.detectMultiScale(
            gray,
            scaleFactor=1.1,
            minNeighbors=5,
            minSize=(MIN_FACE_SIZE, MIN_FACE_SIZE)
        )
        
        detected_faces = []
        for (x, y, w, h) in faces[:MAX_FACES_PER_IMAGE]:
            if w < MIN_FACE_SIZE or h < MIN_FACE_SIZE:
                continue
            
            face_region = image[y:y+h, x:x+w]
            embedding = self._create_embedding(face_region)
            
            detected_faces.append({
                'bbox': [int(x), int(y), int(w), int(h)],
                'confidence': 0.85,
                'embedding': embedding
            })
        
        return detected_faces
    
    def _create_embedding(self, face_region: np.ndarray) -> np.ndarray:
        """
        –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ (–≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ) –ª–∏—Ü–∞
        512-–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º –∏ HOG
        """
        # Resize –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É —Ä–∞–∑–º–µ—Ä—É
        face_resized = cv2.resize(face_region, (128, 128))
        gray_face = cv2.cvtColor(face_resized, cv2.COLOR_BGR2GRAY)
        
        # 1. –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —è—Ä–∫–æ—Å—Ç–∏ (256 –∑–Ω–∞—á–µ–Ω–∏–π)
        hist = cv2.calcHist([gray_face], [0], None, [256], [0, 256])
        hist = hist.flatten()
        hist = hist / (np.sum(hist) + 1e-6)
        
        # 2. LBP (Local Binary Patterns) - —Ç–µ–∫—Å—Ç—É—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        lbp = self._compute_lbp(gray_face)
        lbp_hist = cv2.calcHist([lbp], [0], None, [256], [0, 256])
        lbp_hist = lbp_hist.flatten()
        lbp_hist = lbp_hist / (np.sum(lbp_hist) + 1e-6)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        features = np.concatenate([hist, lbp_hist])
        
        # –î–æ–ø–æ–ª–Ω—è–µ–º –¥–æ 512
        if len(features) < 512:
            features = np.pad(features, (0, 512 - len(features)), 'constant')
        else:
            features = features[:512]
        
        return features.astype(np.float32)
    
    def _compute_lbp(self, gray_image: np.ndarray) -> np.ndarray:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ Local Binary Pattern"""
        rows, cols = gray_image.shape
        lbp = np.zeros((rows - 2, cols - 2), dtype=np.uint8)
        
        for i in range(1, rows - 1):
            for j in range(1, cols - 1):
                center = gray_image[i, j]
                code = 0
                
                if gray_image[i-1, j-1] >= center: code |= 1 << 0
                if gray_image[i-1, j] >= center: code |= 1 << 1
                if gray_image[i-1, j+1] >= center: code |= 1 << 2
                if gray_image[i, j+1] >= center: code |= 1 << 3
                if gray_image[i+1, j+1] >= center: code |= 1 << 4
                if gray_image[i+1, j] >= center: code |= 1 << 5
                if gray_image[i+1, j-1] >= center: code |= 1 << 6
                if gray_image[i, j-1] >= center: code |= 1 << 7
                
                lbp[i - 1, j - 1] = code
        
        return lbp
    
    def extract_embedding(self, image: np.ndarray) -> Optional[Tuple[np.ndarray, Dict]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞"""
        faces = self.detect_faces(image)
        
        if not faces:
            return None
        
        best_face = max(faces, key=lambda f: f['confidence'])
        embedding = normalize_embedding(best_face['embedding'])
        
        x, y, w, h = best_face['bbox']
        face_region = image[y:y+h, x:x+w]
        
        lighting_score = assess_lighting_quality(face_region)
        blur_score = assess_motion_blur(face_region)
        
        metadata = {
            'bbox': [int(x) for x in best_face['bbox']],
            'confidence': float(best_face['confidence']),
            'lighting_score': float(lighting_score),
            'blur_score': float(blur_score),
            'embedding_norm': float(np.linalg.norm(embedding))
        }
        
        return embedding, metadata
    
    def verify_face(
        self,
        image: np.ndarray,
        db_session,
        check_liveness: bool = True,
        frames: Optional[List[np.ndarray]] = None
    ) -> Dict[str, Any]:
        """–í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –ª–∏—Ü–∞"""
        timestamp = datetime.utcnow()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∂–∏–≤–æ—Å—Ç–∏
        liveness_result = LivenessEnum.UNKNOWN
        liveness_details = {}
        
        if check_liveness:
            if frames and len(frames) > 1:
                liveness_result, liveness_details = liveness_detector.check_liveness(frames)
            else:
                liveness_result, liveness_details = liveness_detector.check_liveness([image])
            
            logger.info(f"üîç Liveness: {liveness_result}")
            
            if liveness_result == LivenessEnum.SPOOF:
                return self._create_response(
                    verdict=VerdictEnum.SPOOF,
                    liveness=liveness_result,
                    explain="üö´ –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –ø–æ–¥–¥–µ–ª–∫–∞! –≠—Ç–æ —Ñ–æ—Ç–æ –∏–ª–∏ –≤–∏–¥–µ–æ.",
                    timestamp=timestamp,
                    liveness_details=liveness_details
                )
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
        extraction_result = self.extract_embedding(image)
        
        if extraction_result is None:
            return self._create_response(
                verdict=VerdictEnum.NO_FACE_DETECTED,
                liveness=liveness_result,
                explain="‚ùå –õ–∏—Ü–æ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ",
                timestamp=timestamp,
                liveness_details=liveness_details
            )
        
        probe_embedding, probe_metadata = extraction_result
        
        # –ò—â–µ–º –≤ –±–∞–∑–µ
        match_result = self._search_database(probe_embedding, db_session)
        
        if match_result is None:
            return self._create_response(
                verdict=VerdictEnum.NOT_FOUND,
                liveness=liveness_result,
                explain="‚ùì –≠—Ç–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞ –Ω–µ—Ç –≤ –±–∞–∑–µ!",
                timestamp=timestamp,
                probe_metadata=probe_metadata,
                liveness_details=liveness_details
            )
        
        person, best_similarity, best_photo_id = match_result
        
        if best_similarity >= THRESHOLD_HIGH_CONFIDENCE:
            verdict = VerdictEnum.MATCH
            explain = f"‚úÖ –≠—Ç–æ {person.name}! ({best_similarity*100:.1f}%)"
        elif best_similarity >= THRESHOLD_MEDIUM_CONFIDENCE:
            verdict = VerdictEnum.POSSIBLE_MATCH
            explain = f"ü§î –í–æ–∑–º–æ–∂–Ω–æ {person.name} ({best_similarity*100:.1f}%)"
        else:
            verdict = VerdictEnum.NOT_FOUND
            explain = f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω (–ª—É—á—à–µ–µ: {best_similarity*100:.1f}%)"
        
        return self._create_response(
            verdict=verdict,
            liveness=liveness_result,
            explain=explain,
            timestamp=timestamp,
            probe_metadata=probe_metadata,
            candidate_info={
                'id': person.id_person,
                'name': person.name,
                'similarity': best_similarity,
                'photo_id': best_photo_id
            },
            threshold_used=THRESHOLD_HIGH_CONFIDENCE if verdict == VerdictEnum.MATCH else THRESHOLD_MEDIUM_CONFIDENCE,
            liveness_details=liveness_details
        )
    
    def _search_database(
        self,
        probe_embedding: np.ndarray,
        db_session
    ) -> Optional[Tuple[Person, float, str]]:
        """–ü–æ–∏—Å–∫ –ø–æ –≤–µ–∫—Ç–æ—Ä–∞–º –≤ –±–∞–∑–µ"""
        persons = db_session.query(Person).filter(Person.is_active == True).all()
        
        if not persons:
            logger.info("üì≠ –ë–∞–∑–∞ –ø—É—Å—Ç–∞")
            return None
        
        best_match = None
        best_similarity = -1.0
        best_photo_id = None
        
        for person in persons:
            for embedding_record in person.embeddings:
                ref_embedding = binary_to_embedding(
                    embedding_record.embedding,
                    dtype=np.float32
                )
                
                if len(ref_embedding) != len(probe_embedding):
                    if len(ref_embedding) > len(probe_embedding):
                        ref_embedding = ref_embedding[:len(probe_embedding)]
                    else:
                        ref_embedding = np.pad(
                            ref_embedding,
                            (0, len(probe_embedding) - len(ref_embedding)),
                            'constant'
                        )
                
                similarity = cosine_similarity(probe_embedding, ref_embedding)
                
                logger.info(f"   üìä {person.name}: {similarity*100:.1f}%")
                
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_match = person
                    best_photo_id = embedding_record.photo_id
        
        if best_match:
            logger.info(f"üéØ –õ—É—á—à–∏–π: {best_match.name} ({best_similarity*100:.1f}%)")
        
        return (best_match, best_similarity, best_photo_id) if best_match else None
    
    def enroll_person(
        self,
        images: List[np.ndarray],
        id_person: str,
        name: str,
        db_session,
        metadata: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è —á–µ–ª–æ–≤–µ–∫–∞"""
        if not images:
            raise ValueError("–ù—É–∂–Ω–æ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ")
        
        existing = db_session.query(Person).filter(
            Person.id_person == id_person
        ).first()
        
        if existing:
            raise ValueError(f"–ß–µ–ª–æ–≤–µ–∫ {id_person} —É–∂–µ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω")
        
        embeddings = []
        for idx, image in enumerate(images):
            extraction_result = self.extract_embedding(image)
            
            if extraction_result is None:
                logger.warning(f"‚ö†Ô∏è –õ–∏—Ü–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∞ —Ñ–æ—Ç–æ {idx}")
                continue
            
            embedding, img_metadata = extraction_result
            embeddings.append({
                'embedding': embedding,
                'metadata': img_metadata,
                'photo_id': f"{id_person}_photo_{idx}"
            })
        
        if not embeddings:
            raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ª–∏—Ü")
        
        person = Person(
            id_person=id_person,
            name=name,
            photo_count=len(embeddings),
            person_metadata=metadata or {},
            is_active=True
        )
        db_session.add(person)
        db_session.flush()
        
        for emb_data in embeddings:
            embedding_record = FaceEmbedding(
                person_id=person.id,
                embedding=embedding_to_binary(emb_data['embedding']),
                embedding_norm=float(np.linalg.norm(emb_data['embedding'])),
                photo_id=emb_data['photo_id'],
                quality_metrics=emb_data['metadata']
            )
            db_session.add(embedding_record)
        
        db_session.commit()
        
        logger.info(f"‚úÖ {name} –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω ({len(embeddings)} —Ñ–æ—Ç–æ)")
        
        return {
            'success': True,
            'id_person': id_person,
            'name': name,
            'photo_count': len(embeddings),
            'message': f"‚úÖ {name} –¥–æ–±–∞–≤–ª–µ–Ω!",
            'enrolled_at': person.enrolled_at
        }
    
    def _create_response(
        self,
        verdict: VerdictEnum,
        liveness: LivenessEnum,
        explain: str,
        timestamp: datetime,
        probe_metadata: Optional[Dict] = None,
        candidate_info: Optional[Dict] = None,
        threshold_used: Optional[float] = None,
        liveness_details: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞"""
        response = {
            'verdict': verdict.value,
            'id_candidate': candidate_info['id'] if candidate_info else None,
            'name_candidate': candidate_info['name'] if candidate_info else None,
            'similarity': candidate_info['similarity'] if candidate_info else None,
            'threshold_used': threshold_used,
            'explain': explain,
            'evidence': {
                'probe_face_box': probe_metadata['bbox'] if probe_metadata else [0, 0, 0, 0],
                'probe_embedding_norm': probe_metadata['embedding_norm'] if probe_metadata else 0.0,
                'candidate_stats': {
                    'id': candidate_info['id'],
                    'name': candidate_info['name'],
                    'best_similarity': candidate_info['similarity'],
                    'matched_reference_photo_id': candidate_info['photo_id'],
                    'reference_photo_similarity': candidate_info['similarity']
                } if candidate_info else None,
                'liveness': liveness.value
            },
            'diagnostics': {
                'detector_confidence': probe_metadata['confidence'] if probe_metadata else 0.0,
                'lighting_score': probe_metadata['lighting_score'] if probe_metadata else 0.0,
                'motion_blur_score': probe_metadata['blur_score'] if probe_metadata else 0.0
            },
            'timestamp': timestamp.isoformat() + 'Z'
        }
        
        if liveness_details:
            response['liveness_details'] = liveness_details
        
        return response


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–∏—Å
face_service = SimpleFaceRecognitionService()

